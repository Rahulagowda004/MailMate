{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "af703d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "import os\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_LLM_DEPLOYMENT\"), \n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    )\n",
    "\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"), \n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "76f9639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.path.exists(\"/app/artifacts/embeddings/20250613_124237\"):\n",
    "    persist_directory = \"/app/artifacts/Vector_databases/biology\"\n",
    "else:\n",
    "    persist_directory = r\"R:\\MailMate\\artifacts\\embeddings\\20250613_125644\"\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"resume\",\n",
    "    embedding_function=embedding,\n",
    "    persist_directory=persist_directory,\n",
    ")\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 20}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "45f6a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from operator import add as add_messages\n",
    "from langchain_core.tools import tool "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e72ea070",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retriever_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    This tool searches and returns the information from the document.\n",
    "    \"\"\"\n",
    "    \n",
    "    docs = retriever.invoke(query)\n",
    "    \n",
    "    if not docs:\n",
    "        return \"I found no relevant information in the document.\"\n",
    "    \n",
    "    results = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        results.append(f\"Document {i+1}:\\n{doc.page_content}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b0f29aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool]\n",
    "\n",
    "llm = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4e18ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9d8ff115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: AgentState):\n",
    "    \"\"\"Check if the last message contains tool calls.\"\"\"\n",
    "    result = state['messages'][-1]\n",
    "    return hasattr(result, 'tool_calls') and len(result.tool_calls) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "425b1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "    you are TAZMIC, a research assistant specialized in providing information from a **document**, and provide concise and accurate response in **friendly** and **formal** manner to user queries only based on the content of the document.\n",
    "    \n",
    "    If you do not have enough information to answer the question, you should say \"I don't know\" or \"I found no relevant information in the document.\" instead of making up an answer.\n",
    "\"\"\"\n",
    "\n",
    "tools_dict = {our_tool.name: our_tool for our_tool in tools}\n",
    "tools_dict\n",
    "\n",
    "def call_llm(state: AgentState) -> AgentState:\n",
    "    \"\"\"Function to call the LLM with the current state.\"\"\"\n",
    "    messages = list(state['messages'])\n",
    "    messages = [SystemMessage(content=system_prompt)] + messages\n",
    "    message = llm.invoke(messages)\n",
    "    return {'messages': [message]}\n",
    "\n",
    "# Retriever Agent\n",
    "def take_action(state: AgentState) -> AgentState:\n",
    "    \"\"\"Execute tool calls from the LLM's response.\"\"\"\n",
    "    \n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "    results = []\n",
    "    \n",
    "    for t in tool_calls:\n",
    "        print(f\"Calling Tool: {t['name']} with query: {t['args'].get('query', 'No query provided')}\")\n",
    "        \n",
    "        try:\n",
    "            if t['name'] not in tools_dict:\n",
    "                print(f\"\\nTool: {t['name']} does not exist.\")\n",
    "                result = \"Incorrect Tool Name, Please Retry and Select tool from List of Available tools.\"\n",
    "            else:\n",
    "                result = tools_dict[t['name']].invoke(t['args'].get('query', ''))\n",
    "                print(f\"Result length: {len(str(result))}\")\n",
    "                \n",
    "            # Always append the Tool Message with proper ID\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in tool execution: {e}\")\n",
    "            # Even on error, we must respond to the tool call\n",
    "            error_result = f\"Tool execution failed: {str(e)}\"\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=error_result))\n",
    "\n",
    "    print(\"Tools Execution Complete. Back to the model!\")\n",
    "    return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5c377afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"llm\", call_llm)\n",
    "graph.add_node(\"retriever_agent\", take_action)\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"llm\",\n",
    "    should_continue,\n",
    "    {True: \"retriever_agent\", False: END}\n",
    ")\n",
    "graph.add_edge(\"retriever_agent\", \"llm\")\n",
    "graph.set_entry_point(\"llm\")\n",
    "\n",
    "rag_agent = graph.compile()\n",
    "\n",
    "\n",
    "def query_agent(user_input: str) -> str:\n",
    "    \"\"\"\n",
    "    Process a single user query and return the response.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [HumanMessage(content=user_input)]\n",
    "    \n",
    "    events = rag_agent.stream(\n",
    "        {\"messages\": messages},\n",
    "        stream_mode=\"values\",\n",
    "    )\n",
    "    \n",
    "    final_response = \"\"\n",
    "    for event in events:\n",
    "        last_message = event[\"messages\"][-1]\n",
    "        if hasattr(last_message, 'content') and hasattr(last_message, 'type'):\n",
    "            if last_message.type == \"ai\":\n",
    "                final_response = last_message.content\n",
    "    \n",
    "    return final_response\n",
    "\n",
    "def running_agent():\n",
    "    \"\"\"\n",
    "    Console version of the agent for testing.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== RAG AGENT===\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nWhat is your question: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            break\n",
    "            \n",
    "        response = query_agent(user_input)\n",
    "        print(f\"\\nAssistant: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b789200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RAG AGENT===\n",
      "Calling Tool: retriever_tool with query: Rahul\n",
      "Result length: 3840\n",
      "Tools Execution Complete. Back to the model!\n",
      "\n",
      "Assistant: Rahul A. Gowda is an AI and machine learning professional with experience in research, development, and application of advanced AI technologies. He is currently a Project Trainee at URSC, ISRO, Bangalore, contributing to the development of generative and agentic AI applications, and conducting research on large language models (LLMs). Rahul's expertise encompasses building computer vision models, creating innovative machine learning and AI tools, and developing phishing detection systems.\n",
      "\n",
      "He holds a Bachelor of Engineering in Artificial Intelligence from Vemana Institute of Technology, Bengaluru, Karnataka, and has interned in organizations such as Aspire Technologies, Al-Zira Technologies, and Unifirst Robotics. He actively participates in hackathons, demonstrating leadership skills and excelling in technical challenges, with victories in competitions like VIT Hackathon, Hackmania, and Securathon.\n",
      "\n",
      "Rahul's technical skills include programming in Python, Java, and C, along with proficiency in tools and frameworks like TensorFlow, PyTorch, Docker, and LangGraph. His projects and achievements highlight his capabilities in deploying scalable AI solutions, chatbot development, and implementing explainable AI models. You may explore more of his work on his GitHub: [Rahulagowda004](https://github.com/Rahulagowda004).\n",
      "\n",
      "Assistant: Hello! How can I assist you today? Please feel free to ask any questions related to the document.\n",
      "\n",
      "Assistant: It seems like there might be a typo in your query. Could you clarify what information you're looking for? I'd be glad to help!\n"
     ]
    }
   ],
   "source": [
    "running_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
